{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Extraction, Feature Generation, Dimensionsreduzierung und PCA\n",
    "\n",
    "> __In God we trust, all others bring data__\n",
    "\n",
    ">William Edwards Deming (1900-1993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:22.200722Z",
     "start_time": "2018-11-13T16:47:22.172200Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Einleitung\n",
    "\n",
    "### Format der „Folien“\n",
    "\n",
    "Hierbei handelt es sich um ein sogenanntes Jupyter Notebook.\n",
    "\n",
    "<https://jupyter.org>\n",
    "\n",
    "Eine Mischung aus Code, Bildern und Text mit Teilweise interaktiven Elementen.\n",
    "\n",
    "Auch lokal auf euren Rechnern ausführbar. \n",
    "\n",
    "<img src=\"./ml/images/jupyter.png\" alt=\"Jupyter Logo\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### scikit-learn (sklearn)\n",
    "\n",
    "Programmbeispiele und Plots in der Vorlesung sind mithilfe des `scikit-learn` Projektes erstellt.\n",
    "\n",
    "<http://scikit-learn.org/stable>\n",
    "\n",
    "`scikit-learn` ist eine Bibliothek für Python mit vielen Methoden und Algortihmen für Data Mining / Machine Learning\n",
    "\n",
    "Ausführlicher User-Guide mit Beispielen und mathematischen Hintegründen:\n",
    "<https://scikit-learn.org/stable/user_guide.html>\n",
    "\n",
    "<img src=\"./ml/images/logo.png\" alt=\"Scikit Logo\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "> Pedregosa, Fabian, et al. \"Scikit-learn: Machine learning in Python.\" the Journal of machine Learning research 12 (2011): 2825-2830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### pandas\n",
    "\n",
    "Für die Daten-Vorverarbeitung werden wir hauptsächlich `pandas` verwenden.\n",
    "\n",
    "Pandas stellt den `DataFrame` zur Verfügung, eine tabellenartige Datenstruktur mit vielen nützlichen Methoden\n",
    "\n",
    "<https://pandas.pydata.org>\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ed/Pandas_logo.svg\" alt=\"pandas log\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alle benötigten Pakete sind in der `conda` Umgebung enthalten, zur Erinnerung:\n",
    " \n",
    "Umgebung erstellen:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "\n",
    "Umgebung aktualisieren:\n",
    "```\n",
    "conda env update -f environment.yml\n",
    "```   \n",
    "\n",
    "Umgebung aktivieren\n",
    "```\n",
    "conda activate ml\n",
    "```\n",
    "\n",
    "Notebook Server starten:\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notationen\n",
    "\n",
    "*Für weiteres siehe \"Elements of statistical Learning\" von Trevor Hastie. https://web.stanford.edu/~hastie/ElemStatLearn/ (Kostenloses E-Book)* \n",
    "\n",
    "Ich versuche folgenden Namenskonventionen zu folgen.\n",
    "\n",
    "* Großbuchstaben wie  $X$ oder $Y$ bezeichnen generische Aspekte einer Variable (i.e. die tatsächliche Zufallsvariable)\n",
    "* Beobachtungen/Realisierungen werden klein geschrieben. Die i-te Realisierung in $X$ ist $x_i$\n",
    "* Matrizen sind groß- und fettgedruckt $\\boldsymbol{X}$\n",
    "* Beobachtungen/Realisierungen sind *Zeilen* der Matrix während die beobachteten Größen in den *spalten* stehen.\n",
    "\n",
    "Wenn man beispielsweise $d=2$ Variablen, das Alter und das Gewicht von $N = 100$ Menschen misst, dann erhält man eine $N \\times d$ Matrix $\\boldsymbol{X}$.\n",
    "\n",
    "eine Beobachtung, bzw. Zeile, der Matrix wird geschrieben als $x_i = (\\mathrm{Alter}, \\mathrm{Gewicht} )$.\n",
    "\n",
    "Alle Messungen der Größe *Gewicht* sind geschrieben als  $\\boldsymbol{x}_{\\bullet 1}$, analog zum `numpy` indexing `X[:, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:23.953831Z",
     "start_time": "2018-11-13T16:47:22.205726Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ml import plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:23.953831Z",
     "start_time": "2018-11-13T16:47:22.205726Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plots.set_plot_style()\n",
    "\n",
    "\n",
    "colors = plots.colors\n",
    "cmap = plots.cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wiederholung\n",
    "\n",
    "Beim letzten mal:\n",
    "\n",
    "Lineare Fisher Diskriminanzanalyse:\n",
    "\n",
    "> Finde die Hyperebenen welche zwei Populationen optimal nach Fisher Kriterium trennt.\n",
    "\n",
    "Das kleine Beispiel unten zeigt wie eine Diskriminanzanalyse mit dem scikit-learn Paket durchgeführt werden kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=250, centers=2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608936e5e5074a28bac95a99699ab654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f80cdce8d00>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect(1)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "lda = clf.fit(X, y)\n",
    "\n",
    "projection = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5f87881e4e4bfb930d8cb4cb2a4152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax, ax_proj) = plt.subplots(1, 2)\n",
    "\n",
    "ax.set_aspect(1, 'datalim')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=cmap)\n",
    "plots.draw_linear_regression_function(lda, color='gray', ax=ax)\n",
    "\n",
    "\n",
    "for label, color in zip((0, 1), cmap.colors):\n",
    "    ax_proj.hist(projection[y == label], bins=20, range=[-5, 5], color=color, histtype='step')\n",
    "\n",
    "ax_proj.axvline(0, color='gray')\n",
    "    \n",
    "None # to keep notebook output from cluttering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probleme bei hochdimensionalen Daten:\n",
    "\n",
    "> __Curse of dimensionality (Fluch der Dimensionalität)__\n",
    ">\n",
    "> ist ein Begriff, der von Richard Bellman eingeführt wurde,   \n",
    "> um den rapiden Anstieg im Volumen beim Hinzufügen weiterer Dimensionen in einen mathematischen Raum zu beschreiben.\n",
    ">\n",
    ">[https://de.wikipedia.org/wiki/Fluch_der_Dimensionalität](https://de.wikipedia.org/wiki/Fluch_der_Dimensionalit%C3%A4t)\n",
    "\n",
    "Je höher die Dimension des Raumes, umso mehr Beobachtungen braucht man um den Raum *ausreichend* abzudecken.\n",
    "\n",
    "Im folgenden Beispiel werden 100 Punkte aus eine uniformen Verteilung zwischen 0 und 1 gezogen.  \n",
    "Darunter wird das Histogram gezeichnet.  \n",
    "Zunächst in einer Dimension und dann in zwei.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.073068Z",
     "start_time": "2018-11-13T16:47:24.486586Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "sample = rng.uniform(low=0.0, high=1.0, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.073068Z",
     "start_time": "2018-11-13T16:47:24.486586Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bc2c478ade4075b4ae389cd4a30aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "hist, edges, plot = ax.hist(sample, range=[0, 1], bins=10, histtype='step', lw=2)\n",
    "\n",
    "ax.scatter(sample, np.full_like(sample, 1.1 * hist.max()), color='C0')\n",
    "density = np.count_nonzero(hist) / hist.size\n",
    "\n",
    "ax.set_ylabel('Häufigkeit')\n",
    "ax.set_title('Besetzte Bins: {:.2%}'.format(density))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.897248Z",
     "start_time": "2018-11-13T16:47:25.078702Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36840ce76664b278a58c65cc46ec3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gleichverteilte Zahlen in zwei Dimensionen\n",
    "sample = rng.uniform(low=0.0, high=1.0, size=(100, 2))\n",
    "\n",
    "# Ein ausführliches plotting beispiel.\n",
    "# In Zukunft werden viele Plotting-Funktionen aus dem ml modul importiert und sind nicht direkt im Notebook\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "# Einzelne Punkte plotten\n",
    "ax1.scatter(sample[:, 0], sample[:, 1])\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "\n",
    "# das Histogram plotten\n",
    "hist, _, _, plot = ax2.hist2d(\n",
    "    sample[:, 0],\n",
    "    sample[:, 1],\n",
    "    range=[[0, 1], [0, 1]],\n",
    "    bins=10,\n",
    "    cmap='inferno',\n",
    "    vmin=0,\n",
    ")\n",
    "\n",
    "# Anteil besetzter Bins bestimmen\n",
    "density = np.count_nonzero(hist) / hist.size\n",
    "\n",
    "ax2.set_title('Besetzte Bins: {:.0%}'.format(density))\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "fig.colorbar(plot, ax=ax2)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im 1D Beispiel ist jeder Bin besetzt. Es gibt keine leeren bins. Im 2D Beispiel sind bereits ungefähr ein Drittel der Bins leer. \n",
    "\n",
    "Lösungsmöglichkeiten:\n",
    "\n",
    "1. Mehr Daten Speichern und höhere Kosten in Kauf nehmen.\n",
    "2. Größere Bins benutzen und die Verteilung weniger genau wiedergeben.\n",
    "3. Dimensionen reduzieren und eventuell Informationen verwerfen. \n",
    "\n",
    "Mehr Daten zu speichern ist auch heutzutage nicht immer möglich.\n",
    "\n",
    "### Beispiel IceCube\n",
    "Der IceCube Neutrino Detektor am Südpol nimmt bis zu 1 TB Daten am Tag auf.   \n",
    "Per Satelit können nur 100 GB pro Tag übetragen werden.  \n",
    "Die restlichen Daten werden einmal pro Jahr per Schiff versandt.  \n",
    "\n",
    "<img src=\"./ml/images/icecube.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Beispiel SKA\n",
    "Das Square Kilometer Array ist ein geplantes Radio Teleskop welches in Südafrika und Australien gebaut werden soll.\n",
    "\n",
    "Es wird aus mehreren zehntausenden Antennen bestehen.\n",
    "\n",
    "Die erwartete Datenrate liegt in der Größenordnung von mehreren  __Petabyte pro Sekunde__.\n",
    "\n",
    "Eine Speicherung ist mit heutiger Technologie völlig unmöglich.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.911324Z",
     "start_time": "2018-11-13T16:47:25.900962Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.youtube.com/embed/8BBoDw2qVD0?rel=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f80ec5813a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://www.youtube.com/embed/8BBoDw2qVD0?rel=0', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hochdimensionale Daten führen aber auch zu ganz grundsätzlichen, mathematischen,  Problemen.\n",
    "Interessante Diskussion hier: \n",
    "> Why is Euclidean distance not a good metric in high dimensions?  \n",
    ">https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions\n",
    "\n",
    "Benötigt wird also irgendeine Art von Dimensionsreduktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datenreduktion\n",
    "\n",
    "Zwei grundsätzliche, kombinierbare Ansätze:\n",
    "\n",
    "1. Feature Extraction\n",
    "  * Erzeugen neuer Attribute, die Informationen mehrerer Attribute zusammenfassen\n",
    "  * Automatisches Reduzieren der Dimension durch minimieren der Varianz (Hauptkomponenteanalyse)\n",
    "   \n",
    "1. Feature Selection\n",
    "  * Verwerfen von redundanten und schwachen Attributen\n",
    "  \n",
    "  \n",
    "In der Regel werden wir in Analysen aus Rohdaten neue Attribute erzeugen und dann relevante Attribute auswählen.\n",
    "\n",
    "Dies kann in mehreren Stufen passieren, so das die Rohdaten immer weiter abstrahiert werden und Attribute näher and der physikalischen Fragestellung generiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "#### Beispiel 1: Einfache Datentransformation\n",
    "\n",
    "Gegeben seien Datenpunkte $X = (x_1, x_2, ...) , Y = (y_1, y_2, ...) $.\n",
    "\n",
    "Angenommen durch Fachwissen, bekannte physikalische Zusammenhänge oder einfach genaues hinsehen sei bekannt, \n",
    "dass die Daten sich besser durch Polarkoordinaten ausdrücken lassen.\n",
    "\n",
    "$$\n",
    " \\begin{align}\n",
    "     r &= x^2 + y^2 \\\\\n",
    "     \\phi &= \\operatorname{arctan2}(y, x)    \n",
    " \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:27.177886Z",
     "start_time": "2018-11-13T16:47:25.917299Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def transform(X):\n",
    "    r = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "    phi = np.arctan2(X[:, 1], X[:, 0])\n",
    "    return np.column_stack([r, phi])\n",
    "\n",
    "\n",
    "X_original, y = make_circles(n_samples=500, noise=0.15, factor=0.4, )\n",
    "X_transformed = transform(X_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:27.177886Z",
     "start_time": "2018-11-13T16:47:25.917299Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436c1ccb0ac4425f9499243622a29803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# erstelle eine figure mit zwei subplots\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# plotte die ursprünglichen Punkte\n",
    "\n",
    "for X, ax, title in zip((X_original, X_transformed), axs, ('Original', 'Transformiert')):\n",
    "    \n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    ax.set_title(title)\n",
    "\n",
    "axs[0].set_aspect(1, adjustable='datalim')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In diesem Beispiel is es ausreichend, allein das Attribut $r$ zu speichern um die Daten in zwei Klassen zu \n",
    "unterteilen.\n",
    "\n",
    "Die Dimensionalität reduziert sich also um die Hälfte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Beispiel 2: Datenrepresäntation\n",
    "\n",
    "Häufig sind die Daten nicht in der richtigen Darstellung für die statistischen Methoden die wir anwenden wollen.\n",
    "In diesem Beispiel sollen Textschnippsel aus dem Internet in verschiednen Kategorien eingeteilt werden.\n",
    "Dazu müssen aus den Rohdaten, den einzelnen Texten, irgendwie Variablen abgeleitet werden, die uns einen Hinweis auf die Kategorie geben können.\n",
    "\n",
    "Wir laden zunächst einen Beispieldatensatz mit Texten aus zwei Kategorien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.100707Z",
     "start_time": "2018-11-13T16:47:27.181136Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "atheist_texts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), categories=['alt.atheism'])\n",
    "religous_texts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), categories=['talk.religion.misc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.100707Z",
     "start_time": "2018-11-13T16:47:27.181136Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.100707Z",
     "start_time": "2018-11-13T16:47:27.181136Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aus dem Atheisten Forum:\n",
      "\n",
      "Who has to consider it?  The being that does the action?  I'm still\n",
      "not sure I know what you are trying to say.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aus dem Religions Forum:\n",
      "\n",
      "Nut or not, he was clearly a liar.  He said he would surrender after\n",
      "  local radio stations broadcast his message, but he didn't.  Then he\n",
      "  said he would surrender after Passover, but he didn't.\n",
      "\n",
      "  None of which excuses the gross incompetence and disregard for the\n",
      "  safety of the children displayed by the feds.   As someone else\n",
      "  pointed out, if it had been Chelsea Clinton in there you would \n",
      "  probably have seen more restraint.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Aus dem Atheisten Forum:\\n')\n",
    "print(rng.choice(atheist_texts.data).lstrip())\n",
    "print('\\n'*3)\n",
    "\n",
    "print('Aus dem Religions Forum:\\n')\n",
    "print(rng.choice(religous_texts.data).lstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothese 1__: Atheisten benutzen mehr Wörter\n",
    "\n",
    "Wir füllen die Längen der Texte in normierte Histogramme ein um zu sehen, ob sich die Verteilungen unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.680634Z",
     "start_time": "2018-11-13T16:47:31.103985Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c559ccd84241199ddd401b64e82196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median     : 91 87\n",
      "75%-Quantil: 172, 235\n"
     ]
    }
   ],
   "source": [
    "def number_of_words(text: str):\n",
    "    return len(text.split())\n",
    "\n",
    "atheist_lengths = list(map(number_of_words, atheist_texts.data))\n",
    "religous_lengths = list(map(number_of_words, religous_texts.data))\n",
    "\n",
    "\n",
    "bins =  np.arange(0, 2000, 25)\n",
    "\n",
    "hist_options = dict(\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    "    histtype='step',\n",
    "    lw=4,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(atheist_lengths,  label='Atheist', **hist_options)\n",
    "plt.hist(religous_lengths, label='Religous', **hist_options)\n",
    "plt.xlabel('Text Länge')\n",
    "plt.legend()\n",
    "None\n",
    "\n",
    "print(f'Median     : {np.median(atheist_lengths):.0f} {np.median(religous_lengths):.0f}')\n",
    "print(f'75%-Quantil: {np.percentile(atheist_lengths, 75):.0f}, {np.percentile(religous_lengths, 75):.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keine signifikanten Unterschiede per Auge zu sehen.\n",
    "\n",
    "__Hypothese 2__: Atheisten benutzen längere Wörter\n",
    "\n",
    "Selbes vorgehen wie oben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:32.289809Z",
     "start_time": "2018-11-13T16:47:31.683907Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9438bd3997744dcbb137c90df0e72145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def max_word_length(s):\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return 0\n",
    "    return max(map(len, words))\n",
    "\n",
    "atheist_lengths = list(map(max_word_length, atheist_texts.data))\n",
    "religous_lengths = list(map(max_word_length, religous_texts.data))\n",
    "\n",
    "\n",
    "# Histogrammieren der Wortlängen. \n",
    "bins =  np.arange(0, 100, 1)\n",
    "hist_options = dict(\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    "    histtype='step',\n",
    "    lw=4,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(atheist_lengths, label='Atheists', **hist_options)\n",
    "plt.hist(religous_lengths, label='Religous', **hist_options)\n",
    "plt.legend()\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo kommen die großen Wortlängen her? Stichwort __Datenbereinigung__. Manche der Texte Enthalten Signaturen oder andere Zeichenfolgen die nicht zum Haupttext gehören.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:32.306869Z",
     "start_time": "2018-11-13T16:47:32.294360Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's\",\n",
       " 'not',\n",
       " 'a',\n",
       " 'very',\n",
       " 'good',\n",
       " 'example',\n",
       " 'to',\n",
       " 'show',\n",
       " 'citizenship',\n",
       " 'without',\n",
       " 'descent.',\n",
       " 'Karl',\n",
       " '-----------------------------------------------------------------------------']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atheist_texts.data[2].split()[40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothese 3__: Atheisten benutzen andere Wörter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:33.752517Z",
     "start_time": "2018-11-13T16:47:32.311675Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e560571eac94b82a1e034539828015f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rather atheists belief something evidence religion argument cannot religious\n",
      "against person christians children different doesn't nothing koresh christian\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def extract_words(texts):\n",
    "    return (\n",
    "        ' '.join(texts.data)           # join all texts to one giant string\n",
    "        .lower()                       # all to lower case\n",
    "        .translate(string.punctuation) # remove punctuation ,.- etc.\n",
    "        .split()                       # split sring into list of words at whitespace\n",
    "    )\n",
    "\n",
    "\n",
    "def most_common(words, n, min_length=5):\n",
    "    counter = Counter(filter(lambda w: len(w) > min_length, words))\n",
    "    return dict(counter.most_common(n))\n",
    "\n",
    "\n",
    "atheist_words = extract_words(atheist_texts)\n",
    "common_atheist_words = most_common(atheist_words, 15)\n",
    "\n",
    "religous_words = extract_words(religous_texts)\n",
    "common_religous_words = most_common(religous_words, 15)\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.set_title('Atheists')\n",
    "ax1.barh(list(common_atheist_words.keys()), list(common_atheist_words.values()))\n",
    "\n",
    "ax2.set_title('Religous')\n",
    "ax2.barh(list(common_religous_words.keys()), list(common_religous_words.values()), color='C1')\n",
    "\n",
    "\n",
    "atheist_set = set(common_atheist_words.keys())       \n",
    "religous_set = set(common_religous_words.keys())\n",
    "s = atheist_set | religous_set\n",
    "print(*(atheist_set - religous_set))\n",
    "print(*(religous_set - atheist_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die benutzten Wörter unterscheiden sich also deutlich. Die Anzahl der Wörter die man benutzen kann um die Kategorie über die Wortfrequenz zuzuordnen ist in diesem Beispiel relativ klein. \n",
    "\n",
    "\n",
    "In echten Problemen ist das natürlich wesentlich aufwendiger und man benötigt andere Techniken und mehr Beispiele.\n",
    "\n",
    "\n",
    "Diese Art der Feature Extraction erfordert irgendeine Art von Expertenwissen und wird bei steigender Dimensionalität komplizierter bis unmöglich. \n",
    "\n",
    "Expertenwissen bedeutet häufig, dass bekannte physikalische Zusammenhänge ausgenutzt werden. Manchmal sind es aber gerade diese physikalischen Zusammenhänge die unbekannt sind und gelernt werden sollen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA (Hauptkomponentenanalyse)\n",
    "\n",
    "Die Hauptkomponentenanalyse sucht nach einer Basis im Raum indem die Varianz entlang der Basisvektoren maximiert wird.\n",
    "\n",
    "Gegeben seien also $N$ Datenpunkte mit $d$ Dimensionen die auf $k < d$ Dimensionen transformiert werden sollen.\n",
    "\n",
    "Dazu wird der Raum in eine neue Basis transformiert. Es werden also eventuell mehrere Dimensionen/Attribute zu einer neuen zusammengefasst.  \n",
    "\n",
    "Grober Ablauf der PCA\n",
    "\n",
    "0. Zentriere die Daten auf ihren Mittelwert.\n",
    "1. Berechne die Kovarianzmatrix aus der Datenmatrix $\\boldsymbol{X}$\n",
    "2. Berechne Eigenwerte und Eigenvektoren der Matrix\n",
    "3. Wähle die $k$ größten Eigenwerte und zugehörigen Eigenvektoren aus. \n",
    "4. Bilde eine $d \\times k$ Matrix $\\boldsymbol{W}$ mit den $k$ Eigenvektoren als Spalten.\n",
    "5. Wende $\\boldsymbol{W}$ auf jede Zeile aus $x$ aus $\\boldsymbol{X}$ an $x^\\prime = \\boldsymbol{W}^T \\cdot x^T $ \n",
    "    \n",
    "\n",
    "###### 1. Zentrierung \n",
    "\n",
    "Berechne die Mittelwertvektoren $\\mu$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\begin{pmatrix}\n",
    "    \\bar{\\boldsymbol{x}}_1 \\\\\n",
    "    \\ldots \\\\\n",
    "    \\bar{\\boldsymbol{x}}_d \\\\\n",
    "\\end{pmatrix}\n",
    " = \\frac 1 N\n",
    " \\begin{pmatrix}\n",
    "    \\sum_{i=0}^{N} \\boldsymbol{x}_{1, i} \\\\\n",
    "    \\ldots \\\\\n",
    "    \\sum_{i=0}^{N} \\boldsymbol{x}_{d, i} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Anders ausgedrückt:\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\begin{pmatrix}\n",
    "    \\text{Mittelwert aller Beobachtungen von Attribut 1}\\\\\n",
    "    \\ldots \\\\\n",
    "    \\text{Mittelwert aller Beobachtungen von Attribut d}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Neuer Datenpunkt:\n",
    "\n",
    "   $$\n",
    "      x^{\\prime}_i = x_i - \\boldsymbol{\\mu}\n",
    "   $$\n",
    "\n",
    "###### 2. Kovarianz\n",
    "\n",
    "Die Kovarianz einer Zufallsvariable $X$ beliebiger Dimension.\n",
    "$$\n",
    "\\operatorname {Cov} (X)=\\operatorname {E} {\\bigl [}(X-\\operatorname {E} (X))\\cdot (X-\\operatorname {E} (X))^T{\\bigr ]}\n",
    "$$\n",
    "\n",
    "Schätzung der Kovarianzmatrix auf Daten auch durch *einfache* Matrixoperation möglich. \n",
    "\n",
    "###### 3. Eigenwerte und Vektoren\n",
    "\n",
    "Berechne die $d$ verschiedenen Eigenwerte der Kovarianzmatrix $ \\operatorname {Cov} (\\boldsymbol {X} )$.\n",
    "\n",
    "Erhalte Eigenwerte $\\lambda_1, \\ldots, \\lambda_d$ mit passenden Eigenvektoren $v_1, \\ldots, v_d$\n",
    "\n",
    "\n",
    "###### 4. Sortierung und Auswahl\n",
    "\n",
    "Sortiere die Indizes der Eigenwerte und Vektoren so dass gilt \n",
    "\n",
    "$$\n",
    "\\lambda_1 > \\lambda_2 > \\lambda_3 \\ldots > \\lambda_d\n",
    "$$\n",
    "\n",
    "Wähle die $k$ größten Eigenwerte aus und verwerfe alle anderen Eigenwerte und Vektoren.\n",
    "\n",
    "\n",
    "\n",
    "###### 5. Bildung der Matrix\n",
    "\n",
    "Nutze die $k$ ausgewählten Eigenvektoren als Spalten in der Matrix $\\boldsymbol{W}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W} = \\begin{pmatrix}\n",
    "    v_1, \n",
    "    \\ldots,  \n",
    "    v_k\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "    v_{1,1}, \n",
    "    \\ldots,  \n",
    "    v_{k, 1} \\\\\n",
    "    \\ldots \\\\\n",
    "        v_{1,d}, \n",
    "    \\ldots,  \n",
    "    v_{k, d}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "###### 6. Transformierung \n",
    "\n",
    "Multipliziere die Transformationsmatrix $\\boldsymbol{W}$ mit jeder Beobachtung $x_i$ in $\\boldsymbol{X}$ um die auf $k$ Dimensionen beschränkten Punkte zu erhalten.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^\\prime} = \\boldsymbol{X} \\boldsymbol{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:33.911709Z",
     "start_time": "2018-11-13T16:47:33.755670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.279 0.829 0.483 0.117]\n",
      "[[0.838 0.174 0.212 0.017]\n",
      " [0.174 0.546 0.02  0.064]\n",
      " [0.212 0.02  0.824 0.49 ]\n",
      " [0.017 0.064 0.49  0.5  ]]\n",
      "[[-0.894  0.115  0.446 -0.359]\n",
      " [-1.586 -0.733 -0.348 -0.311]\n",
      " [ 0.621 -0.548  1.015  0.359]\n",
      " [ 1.992 -0.951 -0.452 -0.359]\n",
      " [-0.858 -0.749  0.136  0.396]\n",
      " [-0.191  0.813 -1.186  0.299]\n",
      " [ 0.246  1.626  0.6   -0.222]\n",
      " [ 0.671  0.427 -0.21   0.196]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "N = 8 # anzahl beobachtungen\n",
    "d = 4 # anzahl variablen/ dimensionen\n",
    "\n",
    "X = rng.normal(size=(N, d))\n",
    "X = X - X.mean(axis=0)\n",
    "\n",
    "# print(X.shape)\n",
    "c = np.cov(X, rowvar=False)\n",
    "\n",
    "# eigh garantiert sortierte Eigenwerte, allerdings aufsteigend\n",
    "l, W = np.linalg.eigh(c)\n",
    "\n",
    "# Reihenfolge umkehren. Größte Eigenwerte zuerst.\n",
    "l = l[::-1]\n",
    "W = W[:, ::-1]\n",
    "\n",
    "X_prime = X @ W\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(l)\n",
    "    print(c)\n",
    "    print(X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:33.911709Z",
     "start_time": "2018-11-13T16:47:33.755670Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Hand:\n",
      " [[-0.89  0.12  0.45 -0.36]\n",
      " [-1.59 -0.73 -0.35 -0.31]\n",
      " [ 0.62 -0.55  1.01  0.36]\n",
      " [ 1.99 -0.95 -0.45 -0.36]\n",
      " [-0.86 -0.75  0.14  0.4 ]\n",
      " [-0.19  0.81 -1.19  0.3 ]\n",
      " [ 0.25  1.63  0.6  -0.22]\n",
      " [ 0.67  0.43 -0.21  0.2 ]]\n",
      "sklearn:\n",
      " [[-0.89  0.12 -0.45 -0.36]\n",
      " [-1.59 -0.73  0.35 -0.31]\n",
      " [ 0.62 -0.55 -1.01  0.36]\n",
      " [ 1.99 -0.95  0.45 -0.36]\n",
      " [-0.86 -0.75 -0.14  0.4 ]\n",
      " [-0.19  0.81  1.19  0.3 ]\n",
      " [ 0.25  1.63 -0.6  -0.22]\n",
      " [ 0.67  0.43  0.21  0.2 ]]\n",
      "\n",
      " All close: True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_prime_sklearn = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print('Per Hand:\\n', X_prime)\n",
    "    print('sklearn:\\n', X_prime_sklearn)\n",
    "\n",
    "# testen aller einträge auf gleichheit (bis auf vorzeichen)\n",
    "print('\\n All close:', np.allclose(np.abs(X_prime), np.abs(X_prime_sklearn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Beispiel in 3D\n",
    "\n",
    "Künstlicher Datensatz mit $d = 3$ Dimensionen wird auf $k=2$ Dimensionen reduziert. \n",
    "Der Datensatz wird gezogen aus zwei Gaussverteilungen mit unterschiedlichen Mittelwerten und gleicher Kovarianzmatrix.\n",
    "Die Darstellung unten Zeigt die Punktwolke aus vier Verschiedenen Richtungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:35.613041Z",
     "start_time": "2018-11-13T16:47:33.914685Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68230ae804e34109b6bd77062e5fc016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = make_blobs(n_samples=500, n_features=3, cluster_std=3, random_state=2)\n",
    "plots.plot_3d_views(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:37.396225Z",
     "start_time": "2018-11-13T16:47:35.616379Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8d46e1ba1442cb90134a9ed2314ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "transformed = pca.fit_transform(X)\n",
    "\n",
    "plots.plot_3d_views(transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:38.050103Z",
     "start_time": "2018-11-13T16:47:37.399569Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085947f679a44e0baf3ee96c958b0183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed = pca.fit_transform(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(transformed[:, 0], transformed[:, 1], c=y, cmap=cmap)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Gesichtserkennung \n",
    "\n",
    "Man betrachte jeden einzelnen Pixel eines Bildes als Attribut dessen Wert der Grauwert des Pixels ist.\n",
    "\n",
    "So wird ein Bild zu einem 1D Vektor. Mehrere Bilder ergeben wieder die Datenmatrix $\\boldsymbol{X}$\n",
    "\n",
    "Ein Bild mit $64 \\times 64$ Pixeln wird so zu einem Vektor der Länge $4096$.\n",
    "\n",
    "##### Ein einfacher Gesichtserkennungsalgorithmus\n",
    "\n",
    "Angenommen die Aufgabe wäre die Zuordnung von Fotos aller Studierenden der TU zu deren Namen.   \n",
    "Gesucht wird also eine Funktion die aus einem Foto einen Namen macht. \n",
    "\n",
    "Idee:\n",
    "1. Speichere Bilder aller Studierenden in einer Matrix $\\boldsymbol{X}$ der Dimension $\\text{Anzahl Studierende} \\times \\text{Anzahl Pixel}$ und einen Labelvektor $y$ der Länge $N$ der die Namen (oder Matrikelnummern) enthält.\n",
    "2. Berechne die Distanz $D$ zwischen einem neuen Foto $x_{\\text{neu}}$ zu allen in $\\boldsymbol{X}$ gespeicherten Bildern. \n",
    "3. Gebe zurück das $y_i$ für das $i$ bei dem $D(x_{\\text{neu}}, x_i)$ minimal ist.\n",
    "\n",
    "Probleme: \n",
    " - Alle Bilder zu speichern ist schwierig bis unmöglich. \n",
    " - Die Distanz zu allen Einträgen zu finden kann zu lange dauern.\n",
    " - Wahl des Distanzmaßes in hohen Dimensionen ist nicht trivial.\n",
    " \n",
    "##### Eigenfaces \n",
    "\n",
    "*Original Artikel von 1991 von Turk und Pentland http://www.mitpressjournals.org/doi/10.1162/jocn.1991.3.1.71*\n",
    "\n",
    "Der Input in den Algorithmus ist der selbe wie oben. Die Matrix aller Fotos $\\boldsymbol{X}$. Diesmal wird diese Matrix jedoch nicht komplett abgespeichert.\n",
    "\n",
    "Idee:\n",
    "1. Wende PCA auf $\\boldsymbol{X}$ an. \n",
    "2. Erhalte Transformationsmatrix $\\boldsymbol{W}$ der Dimension $d \\times k$\n",
    "3. Berechne Gewichte $g_m = \\boldsymbol{v}_m^T \\cdot(x_i - \\boldsymbol{\\mu}) $ für jedes gespeicherte Bild $x_i$ und jeden Eigenvektor $\\boldsymbol{v}_m$ mit $m \\in \\{1, \\ldots, k\\}$ und erhalte so einen Gewichtsvektor $G$ der länge $k$.\n",
    "4. Berechne Distanz $D$ zwischen dem Gewichtsvektor eines neuen Bildes $G_{\\text{neu}}$ zur allen Gewichtsvektoren in der alten Bilder aus. \n",
    "5. Gebe zurück das $y_i$ für das $i$ bei dem $D(G_{\\text{neu}}, G^{i})$ minimal ist.\n",
    "\n",
    "In der Realität ist die Berechnung der PCA auf großen Matrizen nicht immer Trivial.\n",
    "\n",
    "###### Python Beispiel für Eigenfaces\n",
    "\n",
    "Der LFW (Labeled Faces in the Wild) Datensatz ist ein beliebter Datesatz für Algorithmen zur Gesichtserkennung.\n",
    "http://vis-www.cs.umass.edu/lfw/\n",
    "Er enthält etwa 13.000 Bilder von mehreren hundert Personen die aus dem Internet heruntergeladen wurden.\n",
    "\n",
    "Für dieses Beispiel gucken wir uns nur 200 der Bilder an um Rechenzeit zu sparen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:39.027898Z",
     "start_time": "2018-11-13T16:47:38.055271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4324, 7500), (4324,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=10, resize=0.8)\n",
    "\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "names = lfw_people.target_names\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdullah Gul 19\n",
      "Adrien Brody 12\n",
      "Alejandro Toledo 39\n",
      "Alvaro Uribe 35\n",
      "Amelie Mauresmo 21\n",
      "Andre Agassi 36\n",
      "Andy Roddick 15\n",
      "Angelina Jolie 20\n",
      "Ann Veneman 11\n",
      "Anna Kournikova 12\n",
      "Ari Fleischer 13\n",
      "Ariel Sharon 77\n",
      "Arnold Schwarzenegger 42\n",
      "Atal Bihari Vajpayee 24\n",
      "Bill Clinton 29\n",
      "Bill Gates 17\n",
      "Bill McBride 10\n",
      "Bill Simon 15\n",
      "Britney Spears 14\n",
      "Carlos Menem 21\n",
      "Carlos Moya 19\n",
      "Catherine Zeta-Jones 11\n",
      "Charles Moose 13\n",
      "Colin Powell 236\n",
      "Condoleezza Rice 11\n",
      "David Beckham 31\n",
      "David Nalbandian 14\n",
      "Dick Cheney 14\n",
      "Dominique de Villepin 15\n",
      "Donald Rumsfeld 121\n",
      "Edmund Stoiber 13\n",
      "Eduardo Duhalde 14\n",
      "Fidel Castro 18\n",
      "George HW Bush 13\n",
      "George Robertson 22\n",
      "George W Bush 530\n",
      "Gerhard Schroeder 109\n",
      "Gloria Macapagal Arroyo 44\n",
      "Gonzalo Sanchez de Lozada 12\n",
      "Gordon Brown 13\n",
      "Gray Davis 26\n",
      "Guillermo Coria 30\n",
      "Halle Berry 16\n",
      "Hamid Karzai 22\n",
      "Hans Blix 39\n",
      "Harrison Ford 12\n",
      "Hillary Clinton 14\n",
      "Howard Dean 12\n",
      "Hu Jintao 15\n",
      "Hugo Chavez 71\n",
      "Ian Thorpe 10\n",
      "Igor Ivanov 20\n",
      "Jack Straw 28\n",
      "Jackie Chan 13\n",
      "Jacques Chirac 52\n",
      "Jacques Rogge 10\n",
      "James Blake 14\n",
      "James Kelly 11\n",
      "Jason Kidd 10\n",
      "Javier Solana 10\n",
      "Jean Charest 17\n",
      "Jean Chretien 55\n",
      "Jean-David Levitte 10\n",
      "Jeb Bush 12\n",
      "Jennifer Aniston 21\n",
      "Jennifer Capriati 42\n",
      "Jennifer Garner 12\n",
      "Jennifer Lopez 21\n",
      "Jeremy Greenstock 24\n",
      "Jiang Zemin 20\n",
      "Jiri Novak 11\n",
      "Joe Lieberman 13\n",
      "John Allen Muhammad 11\n",
      "John Ashcroft 53\n",
      "John Bolton 17\n",
      "John Howard 19\n",
      "John Kerry 17\n",
      "John Negroponte 31\n",
      "John Paul II 11\n",
      "John Snow 17\n",
      "Joschka Fischer 19\n",
      "Jose Maria Aznar 23\n",
      "Juan Carlos Ferrero 28\n",
      "Julianne Moore 19\n",
      "Julie Gerberding 15\n",
      "Junichiro Koizumi 60\n",
      "Keanu Reeves 12\n",
      "Kim Clijsters 14\n",
      "Kim Ryong-sung 11\n",
      "Kofi Annan 32\n",
      "Lance Armstrong 18\n",
      "Laura Bush 41\n",
      "Lindsay Davenport 22\n",
      "Lleyton Hewitt 41\n",
      "Lucio Gutierrez 13\n",
      "Luiz Inacio Lula da Silva 48\n",
      "Mahathir Mohamad 14\n",
      "Mahmoud Abbas 29\n",
      "Mark Philippoussis 11\n",
      "Megawati Sukarnoputri 33\n",
      "Meryl Streep 15\n",
      "Michael Bloomberg 20\n",
      "Michael Jackson 12\n",
      "Michael Schumacher 18\n",
      "Mike Weir 11\n",
      "Mohammad Khatami 10\n",
      "Mohammed Al-Douri 15\n",
      "Muhammad Ali 10\n",
      "Nancy Pelosi 15\n",
      "Naomi Watts 22\n",
      "Nestor Kirchner 37\n",
      "Nicanor Duarte Frutos 11\n",
      "Nicole Kidman 19\n",
      "Norah Jones 15\n",
      "Paradorn Srichaphan 10\n",
      "Paul Bremer 20\n",
      "Paul Burrell 11\n",
      "Paul Wolfowitz 10\n",
      "Pervez Musharraf 18\n",
      "Pete Sampras 22\n",
      "Pierce Brosnan 15\n",
      "Queen Elizabeth II 13\n",
      "Recep Tayyip Erdogan 30\n",
      "Renee Zellweger 17\n",
      "Ricardo Lagos 27\n",
      "Richard Gephardt 11\n",
      "Richard Gere 10\n",
      "Richard Myers 18\n",
      "Roger Federer 14\n",
      "Roh Moo-hyun 32\n",
      "Rubens Barrichello 12\n",
      "Rudolph Giuliani 26\n",
      "Saddam Hussein 23\n",
      "Salma Hayek 13\n",
      "Serena Williams 52\n",
      "Sergey Lavrov 11\n",
      "Sergio Vieira De Mello 11\n",
      "Silvio Berlusconi 33\n",
      "Spencer Abraham 17\n",
      "Taha Yassin Ramadan 15\n",
      "Tang Jiaxuan 11\n",
      "Tiger Woods 23\n",
      "Tim Henman 19\n",
      "Tom Cruise 10\n",
      "Tom Daschle 25\n",
      "Tom Hanks 10\n",
      "Tom Ridge 33\n",
      "Tommy Franks 16\n",
      "Tommy Thompson 10\n",
      "Tony Blair 144\n",
      "Trent Lott 16\n",
      "Venus Williams 17\n",
      "Vicente Fox 32\n",
      "Vladimir Putin 49\n",
      "Walter Mondale 10\n",
      "Wen Jiabao 13\n",
      "Winona Ryder 24\n",
      "Yoriko Kawaguchi 14\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(lfw_people.target_names):\n",
    "    print(name, np.bincount(y)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lfw_people.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:39.027898Z",
     "start_time": "2018-11-13T16:47:38.055271Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9358342ba9a4588871d56b9ce3f688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 15\n",
    "\n",
    "h, w = lfw_people.images[0].shape\n",
    "img = X[index].reshape(h, w)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(rf'Bild {index} aus $\\mathbf{{\\mathit{{X}}}}$ ({names[y[index]]})')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:39.598180Z",
     "start_time": "2018-11-13T16:47:39.031942Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4226dcf1ab747c5a497b87e8ce5c518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Durschnittliches Gesicht')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_face = X.mean(axis=0).reshape(h, w)\n",
    "plt.figure()\n",
    "plt.imshow(mean_face, cmap='gray')\n",
    "plt.title('Durschnittliches Gesicht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:40.141860Z",
     "start_time": "2018-11-13T16:47:39.601653Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:41.150702Z",
     "start_time": "2018-11-13T16:47:40.147771Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df4373b5e6b426890c95b56da48c0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Eigenface zu Eigenwert 98')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.imshow(eigenfaces[0], cmap='gray')\n",
    "ax1.set_title('Eigenface zu Eigenwert 0')\n",
    "\n",
    "ax2.imshow(eigenfaces[n_components - 2], cmap='gray')\n",
    "ax2.set_title(f'Eigenface zu Eigenwert {n_components - 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Gesichtserkennung\n",
    "\n",
    "\n",
    "Wir nehmen ein Testbild aus dem Datensatz und gucken, ob wir ein zugehöriges Gesicht finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf34f8af744435ad6ca182eff8908e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f80d7713a30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rng.choice(len(X))\n",
    "test_img = X[index]\n",
    "test_name = names[y[index]]\n",
    "\n",
    "X_rest = np.delete(X, index, axis=0)\n",
    "y_rest = np.delete(y, index)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'Bild {index} ({test_name})')\n",
    "plt.imshow(test_img.reshape(h, w), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eba0481f72a4ddeb23b2dae1ba08e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for this large an input, sklearn uses a randomized pca by default\n",
    "# make sure to use the normal one we introduced above (slower)\n",
    "pca = PCA(n_components=n_components, svd_solver='full')\n",
    "\n",
    "X_trafo = pca.fit_transform(X_rest)\n",
    "test_trafo = pca.transform(test_img[np.newaxis, :])\n",
    "\n",
    "best_match = np.argmin(np.linalg.norm(X_trafo - test_trafo, axis=1))\n",
    "\n",
    "prediction = names[y_rest[best_match]]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X_rest[best_match].reshape(h, w), cmap='gray')\n",
    "plt.title(f'Best Match: {best_match} ({prediction})')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nachteile der PCA\n",
    "\n",
    "Intepretierbarkeit?\n",
    "\n",
    "Gerade in physikalischen Problemstellungen schwer nachvollziehbare neue Features.\n",
    "\n",
    "\n",
    "Annahme: wir messen Energie $E$, Zeit $t$ und Koordinaten $x$ und $y$?\n",
    "\n",
    "* Was bedeutet eine Hauptkomponente die sich zu\n",
    "  $$\n",
    "    0.78 \\cdot E - 0.23 \\cdot t + 0.8 \\cdot x - 0.2 \\cdot y\n",
    "  $$\n",
    "  berechnet?\n",
    "\n",
    "* Einheiten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \n",
    "\n",
    "Eine anderer Ansatz zur Dimensionsreduzierung. Dabei werden die Daten nicht Transformiert sondern einfach Spalten entfernt die keine/wenig Aussagekraft besitzen. Die Aussagekraft bezieht dabei auf ein zu Lösendes Klassifizierungsproblem. Welche Spalten man verwerfen kann, ist also sehr Problemspezifisch. Im Allgemeinen folgen viele Feature Selection Ansätze der Heuristik:\n",
    "\n",
    "> Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other.\n",
    "> \n",
    "> -- Mark Hall\n",
    "\n",
    "### Univariate Feature Selection\n",
    "\n",
    "Betrachtet jedes Attribut für sich alleinstehend. Häufig beinhaltet das auch eine Art der händischen Vorverarbeitung.\n",
    "\n",
    "#### Korrelation mit der Zielgröße\n",
    "\n",
    "Angenommen man wolle eine Größe $y$ aus einem 4D Datensatz schätzen. Ein Teil dieser Daten sind aber mit hohem Rauschen versehen oder haben mit der Zielgröße keinen Kausalen oder Statistischen Zusammenhang.\n",
    "\n",
    "Ein einfacher Algorithmus sucht einfach nach den $k$ Attributen mit denhöchsten Korrelationen und entfernt alle anderen.\n",
    "\n",
    "In dem Beispiel unten wird ein Datensatz erzeugt in welchem nur zwei der 4 vorhandenen Attribute mit der Zielgröße $y$ korrelieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:42.531810Z",
     "start_time": "2018-11-13T16:47:41.154671Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42965ce268b6466a9cf8eb4423679dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "X, y = make_regression(n_samples=300, n_features=4, n_informative=2, n_targets=1, random_state=0, noise=0.1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axs.flat, X.T)):\n",
    "    ax.scatter(col, y)\n",
    "    ax.set_ylabel('Target Variable')\n",
    "    ax.set_xlabel('X{}'.format(i))\n",
    "    \n",
    "    r, _ = pearsonr(X[:, i], y)\n",
    "    ax.set_title(rf'Korrelation $\\rho = {r:.3f}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Feature Selection\n",
    "\n",
    "Testet Kombinationen und ganze Untermengen aus Attributen nach verschiedenen Kriterien. Zum Beispiel Korrelation, Mutual Information, Kreuzentropie, Minimal Desciption Length oder einfach die Qualität der Klassifikation/Trennung. \n",
    "\n",
    "\n",
    "Es ist im allgemeinen nicht möglich alle Kombinationen von Attributen zu testen. Die Anzahl der möglichen Kombinationen ist exponentiell. Bei $n$ Attributen müssen nach Binomischen Lehrsatz Kombinationen getestet werden:\n",
    "\n",
    "$$\n",
    "N = \\sum_{k = 1}^{n} \\begin{pmatrix}\n",
    "    n\\\\\n",
    "    k\\\\\n",
    "    \\end{pmatrix} = 2^n -1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Miteinander Korrelierte Attribute \n",
    "\n",
    "In dem Beispiel unten sieht man schnell, dass zwei Attribute miteinander korrelieren. Eines der beiden Attribute ist also überflüssig bzw. redundant. \n",
    "\n",
    "Um miteinander korrelierte Attribute zu finden kann man einfach alle Paare von Attributen miteinander verlgeichen. Der verlgeich von allen Paaren miteinander führt zur quadratischen Laufzeit dieser Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:44.534634Z",
     "start_time": "2018-11-13T16:47:42.537230Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306e674f7c3b436bb8b3b261494b1699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korrelation zwischen Attribut 1 und 2 : -0.21\n",
      "Korrelation zwischen Attribut 1 und 3 : 0.95\n",
      "Korrelation zwischen Attribut 2 und 3 : 0.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=3, n_informative=2, n_redundant=1, n_repeated=0, n_classes=2, n_clusters_per_class=2, random_state=0)\n",
    "plots.plot_3d_views(X, y)\n",
    "\n",
    "for i, j in combinations(range(3), 2):\n",
    "    r, p = pearsonr(X[:, i], X[:, j])\n",
    "    print('Korrelation zwischen Attribut {} und {} : {:.2f}'.format(i + 1, j + 1, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt Selektionsstrategien und Heuristiken um weniger Kombinitionen zu testen. Die meisten beschränken sich darauf nur Paare von Kombinationen zu testen. \n",
    "Diese art von Heuristik wird deshalb häufig auch als Bivariat bezeichnet.\n",
    "\n",
    "Die beiden einfachsten oder bekanntesten Stretegien sind Forward beziehungsweise Backward Selection\n",
    "\n",
    "\n",
    "###### Forward Selection:\n",
    "\n",
    "Das Verfahren arbeitet iterativ und testet Untermengen nach einem festzulegenden Kriterium.\n",
    "Starte mit dem einzelnen besten Attribut $f_0$ und füge so lange Attribute hinzu bis ein Abbruchkriterium erreicht ist. \n",
    "\n",
    "###### Backward Selection:\n",
    "\n",
    "Wie Forward Selection aber es wird von der vollen Menge an Attributen gestartet und dann iterativ Einträge entfernt.\n",
    "\n",
    "#### Max-Relevance, Min-Redundancy (mRMR)\n",
    "\n",
    "Original veröffentlichung von Peng et al. (2005): [ieeexplore.ieee.org/document/1453511/](ieeexplore.ieee.org/document/1453511/)\n",
    "\n",
    "Wähle die Untermenge an Attributen $S_k = \\{f_1, f_2, \\ldots, f_k\\}$ die Insgesamt die höchste Relevanz bezüglich der Zielvariable $y$ hat und gleichzeitig die Korrelation zwischen den Attributen in $S_k$ möglichst klein ist.\n",
    "\n",
    "Die Relevanz wird häufig durch ein Korrelationsmaß oder die sogenannte Mutual Information bestimmt.\n",
    "Mehr zum Thema Mutual Information folgt noch. \n",
    "\n",
    "Für das gesuchte $S_k$ soll gelten $\\max _{S_{k}}(D - R)$, wobei\n",
    "\n",
    "\\begin{align}\n",
    "D(S, y) =& {\\frac {1}{|S|}}\\sum _{f_{i}\\in S}I(f_{i}, y) \\\\\n",
    "R(S)   =& {\\frac {1}{|S|^{2}}}\\sum _{f_{i},f_{j}\\in S}I(f_{i}, f_{j}) \n",
    "\\end{align}\n",
    "\n",
    "Die Untermengen werden gebildet wie bei Forward Selection. Das nächste Attribut wird ausgewählt nach \n",
    "\n",
    "$$\n",
    "\\mathrm {mRMR} =\\max _{S}\\left[{\\frac {1}{|S|}}\\sum _{f_{i}\\in S}I(f_{i}, y)-{\\frac {1}{|S|^{2}}}\\sum _{f_{i},f_{j}\\in S}I(f_{i}, f_{j})\\right].\n",
    "$$\n",
    "\n",
    "Der mRMR Algorithmus gehört zu einer Klasse von Algorithmen die versuchen Relevanz zu maximieren und Redundanz zu minimieren. Er hat einige interessante Eigenschaften aus informationstheoretischer Sicht und ist vor allem in Anwendungen der Biologie und Genetik interessant.\n",
    "\n",
    "\n",
    "**Feature Selection ist vor allem dann Wichtig wenn die Anzahl der Attribute größer ist als die Anzahl der Beispiele im Datensatz.**\n",
    "\n",
    "### Probleme\n",
    "\n",
    "Algorithmen wie mRMR die sich auf iterative Auswahlen verlassen, indem sie zum Beispiel Forward Selection benutzen, werden auch \"greedy\" Heuristiken genannt.\n",
    "\n",
    "Es ist nicht immer gewährleistet, dass auch das globale Optimum erreicht wird wenn eine greedy Heuristik benutzt wird.\n",
    "\n",
    "Das hängt natürlich von der zu optimierenden Zielfunktion ab.\n",
    "\n",
    "Im allgemeinen ist es schwierig bzw. unmöglich in annehmbarer Zeit die \"optimale\" Untermenge an Attributen zu finden.\n",
    "\n",
    "Interresanter Artikel:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multivariate_mutual_information\n",
    "\n",
    "\n",
    "<p style=\"color:gray\"> Für die Theoretiker: Das VERTEX-COVER Problem kann auf MIN_FEATURE reduziert werden. Dadurch wird es NP-Vollständig. http://scottdavies.net/aaai94.pdf </p> \n",
    "\n",
    "\n",
    "Alle hier angeführten Algorithmen gehen davon aus, dass ungeeignete Attribute schon entfernt wurden, z.B.:\n",
    "\n",
    "* Simulations-Attribute\n",
    "* Attribute mit großen Unterschieden zwischen gemessenen und simulierten Daten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
